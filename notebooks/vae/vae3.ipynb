{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import torch.backends.cudnn as cudnn\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/zacheberhart/Convolutional-Disentangled-Variational-Autoencoder/blob/master/Convolutional%20Disentangled%20Variational%20Autoencoder%20(%CE%B2-VAE).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCH = 10\n",
    "# BATCH_SIZE = 32\n",
    "# LR = 0.005\n",
    "path = '/Users/ruchira/Documents/lvl_4_project/lvl4-hons-project/notebooks/train_test_imbal/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt(path + 'X_train.csv', delimiter=',')\n",
    "y_train = np.loadtxt(path + 'y_train.csv', delimiter=',')\n",
    "X_test = np.loadtxt(path + 'X_test.csv', delimiter=',')\n",
    "y_test = np.loadtxt(path + 'y_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109381, 430)\n",
      "(109381,)\n",
      "(36461, 430)\n",
      "(36461,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109381, 431)\n"
     ]
    }
   ],
   "source": [
    "train_arr = np.hstack((X_train, y_train.reshape(-1,1)))\n",
    "print(train_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_Xtrain = torch.from_numpy(X_train)\n",
    "tensor_ytrain = torch.from_numpy(y_train)\n",
    "\n",
    "train_data = Data.TensorDataset(tensor_Xtrain, tensor_ytrain)\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109381\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = False\n",
    "\n",
    "ZDIMS = 10\n",
    "# BETA = 5\n",
    "LR = 1e-3\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "SEED = 4\n",
    "LOG_INTERVAL = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "device = torch.device('cuda' if CUDA else 'cpu')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NC = 1    # channels\n",
    "NEF = 64  # init encoding filters\n",
    "NDF = 64  # init decoding filters\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, zdims):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.zdims = zdims\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            \n",
    "            # input is (NC) x 28 x 28 (MNIST)\n",
    "            nn.Conv1d(in_channels = NC, out_channels = NEF, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # conv layer 2\n",
    "            nn.Conv1d(in_channels = NEF, out_channels = NEF * 2, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm1d(NEF * 2),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # conv layer 3\n",
    "            nn.Conv1d(in_channels = NEF * 2, out_channels = NEF * 4, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm1d(NEF * 4),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # conv layer 4 \n",
    "            nn.Conv1d(in_channels = NEF * 4, out_channels = 1024, kernel_size = 3, stride = 1, padding = 1),\n",
    "            #nn.BatchNorm2d(1024), # OPTIONAL\n",
    "            nn.ReLU(inplace = True)\n",
    "\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "\n",
    "            # input is Z (post-fc)\n",
    "            nn.ConvTranspose1d(in_channels = 1024, out_channels = NDF * 8, kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(NDF * 8),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # deconv layer 2\n",
    "            nn.ConvTranspose1d(in_channels = NDF * 8, out_channels = NDF * 4, kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(NDF * 4),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # deconv layer 3\n",
    "            nn.ConvTranspose1d(in_channels = NDF * 4, out_channels = NDF * 2, kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(NDF * 2),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # deconv layer 4\n",
    "            nn.ConvTranspose1d(in_channels = NDF*2, out_channels = NC, kernel_size = 1, stride = 1),\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "        \n",
    "        # conv fc\n",
    "        self.fc11 = nn.Linear(64, self.zdims) # mu\n",
    "        self.fc12 = nn.Linear(64, self.zdims) # logvar\n",
    "        \n",
    "        # deconv fc\n",
    "        self.fc2  = nn.Linear(self.zdims, 64)\n",
    "    \n",
    "    \n",
    "    def encode(self, x):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        conv = self.encoder(x)\n",
    "        conv = conv.view(-1, 64) # this maybe the problem\n",
    "        mu = self.fc11(conv)\n",
    "        logvar = self.fc12(conv)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def decode(self, z):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        deconv_input = F.relu(self.fc2(z))\n",
    "        deconv_input = deconv_input.view(64,1024,430) # world models: [-1, 1, 1, 1024] # and this\n",
    "        recon_x = self.decoder(deconv_input)\n",
    "        return recon_x\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(logvar * 0.5)\n",
    "        eps = torch.rand_like(std)\n",
    "        z = eps.mul(std).add(mu)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "#         print(z.shape, \"z, forward\")\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_data = Variable(tensor_Xtrain[:5].view(-1, 430).type(torch.FloatTensor)/255.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(zdims = ZDIMS).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar, beta = 1):\n",
    "    '''Use a beta value of 1 for a vanilla VAE'''\n",
    "    print(recon_x.shape, \"recon_x\")\n",
    "    print(x.shape, \"x\")\n",
    "    # loss\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction = 'sum')\n",
    "    \n",
    "    # KL Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return BCE + (beta * KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, beta = 1):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "#         print(data.shape)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar, Z = model(data.view(64,1,430).type(torch.FloatTensor))\n",
    "#         print(recon_batch.shape, \"recon_batch\")\n",
    "#         print(mu.shape, \"mu\")\n",
    "#         print(Z.shape, \"Z\")\n",
    "        \n",
    "        loss = loss_function(recon_batch, data.view(64,1,430).type(torch.FloatTensor), mu, logvar, beta)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(data),\n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)\n",
    "            ))\n",
    "    \n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch,\n",
    "        train_loss / len(train_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70,)\n",
      "[[[-9.02104472 -4.87034136  5.6664161   5.02689224 -3.81840654\n",
      "    4.80226299  9.85344875  7.96805686 -1.46083574  8.17656189]\n",
      "  [-5.73583059 -0.66716076  1.16315956 -5.63071543  5.66644351\n",
      "   -7.40030951 -3.85263415  4.79828062 -5.22617693 -1.40637909]\n",
      "  [-9.3395182   3.73533319 -1.14249692  7.58256298 -8.20788099\n",
      "   -4.37486144 -3.43005465 -3.09681348 -8.68494841  7.9679352 ]\n",
      "  [ 1.51596407  9.0995712  -6.200206   -8.26873124 -1.24161429\n",
      "   -7.00308771  9.27962586  3.15391634  9.92389149 -0.76663836]\n",
      "  [-8.59367473  2.89580179  1.48795639  9.89863293 -9.48271097\n",
      "    3.759304    2.74061341  4.89167158  7.8500397  -8.46366966]\n",
      "  [ 2.43168075 -5.35326252 -6.58658741 -9.37099351  3.42901237\n",
      "    6.87150977 -5.708008    1.46227475 -4.62646994 -4.62756204]\n",
      "  [ 8.23975261  9.12814875 -3.11921754  3.06148022 -4.4520757\n",
      "   -1.65748303 -7.06169745  3.68722782 -5.06723375  4.22871306]]]\n",
      "(1, 7, 10)\n"
     ]
    }
   ],
   "source": [
    "Z = np.random.uniform(-10, 10, 70)\n",
    "print(Z.shape)\n",
    "X = np.random.uniform(-10, 10, 70).reshape(1, 7,-1)\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
